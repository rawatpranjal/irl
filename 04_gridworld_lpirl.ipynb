{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9301df-0fd3-4b7d-aff0-ad156e041274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define the GridWorld environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size  # Grid size (5x5)\n",
    "        self.features = np.zeros((size, size))\n",
    "        self.payoffs = {}\n",
    "        self.define_features()\n",
    "        self.states = [(x, y) for x in range(size) for y in range(size)]\n",
    "    \n",
    "    def define_features(self):\n",
    "        # Define features and their payoffs\n",
    "        # Feature 1: Payoff = 1\n",
    "        # Feature 2: Payoff = 0.95\n",
    "        # Feature 3: Payoff = -1\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if (x + y) % 3 == 0:\n",
    "                    self.features[x, y] = 1  # Feature 1\n",
    "                elif (x + y) % 3 == 1:\n",
    "                    self.features[x, y] = 2  # Feature 2\n",
    "                else:\n",
    "                    self.features[x, y] = 3  # Feature 3\n",
    "        self.payoffs = {1: 1, 2: 0.95, 3: -1}\n",
    "    \n",
    "    def get_payoff(self, position):\n",
    "        x, y = position\n",
    "        feature = self.features[x, y]\n",
    "        return self.payoffs[feature]\n",
    "\n",
    "# Define the Agent class\n",
    "class Agent:\n",
    "    def __init__(self, gridworld, discount_factor=0.95):\n",
    "        self.gridworld = gridworld\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = ['up', 'down', 'left', 'right', 'stay']\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.states = gridworld.states\n",
    "        self.state_indices = {state: idx for idx, state in enumerate(self.states)}\n",
    "        self.num_states = len(self.states)\n",
    "    \n",
    "    def move(self, position, action):\n",
    "        x, y = position\n",
    "        if action == 'up' and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 'down' and x < self.gridworld.size - 1:\n",
    "            x += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 'right' and y < self.gridworld.size - 1:\n",
    "            y += 1\n",
    "        # 'stay' or invalid moves result in no change\n",
    "        return (x, y)\n",
    "    \n",
    "    def value_iteration(self, max_iter=1000, tol=1e-7):\n",
    "        V = np.zeros(self.num_states)\n",
    "        for iteration in range(max_iter):\n",
    "            V_prev = V.copy()\n",
    "            for idx, state in enumerate(self.states):\n",
    "                action_values = []\n",
    "                for action in self.actions:\n",
    "                    new_state = self.move(state, action)\n",
    "                    reward = self.gridworld.get_payoff(new_state)\n",
    "                    new_state_idx = self.state_indices[new_state]\n",
    "                    Q_value = reward + self.discount_factor * V_prev[new_state_idx]\n",
    "                    action_values.append(Q_value)\n",
    "                # Correctly update V(s) using the Bellman equation\n",
    "                V[idx] = logsumexp(action_values)\n",
    "            # Check for convergence\n",
    "            if np.max(np.abs(V - V_prev)) < tol:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "\n",
    "    \n",
    "    def get_action_probabilities(self, state, V):\n",
    "        action_values = []\n",
    "        for action in self.actions:\n",
    "            new_state = self.move(state, action)\n",
    "            reward = self.gridworld.get_payoff(new_state)\n",
    "            new_state_idx = self.state_indices[new_state]\n",
    "            Q_value = reward + self.discount_factor * V[new_state_idx]\n",
    "            action_values.append(Q_value)\n",
    "        # Compute choice probabilities using softmax\n",
    "        log_probs = action_values - logsumexp(action_values)\n",
    "        probs = np.exp(log_probs)\n",
    "        return probs\n",
    "\n",
    "# Function to generate a trajectory based on the process type\n",
    "def generate_trajectory(agent, start_position, process_type='preference_shocks', max_steps=10):\n",
    "    trajectory = []\n",
    "    position = start_position\n",
    "    # Compute the true value function using true parameters\n",
    "    agent.gridworld.payoffs = {1: 1, 2: 0.95, 3: -1}\n",
    "    V = agent.value_iteration()\n",
    "    for step in range(max_steps):\n",
    "        probs = agent.get_action_probabilities(position, V)\n",
    "        action = np.random.choice(agent.actions, p=probs)\n",
    "        new_position = agent.move(position, action)\n",
    "        reward = agent.gridworld.get_payoff(new_position)\n",
    "        trajectory.append((position, action, reward))\n",
    "        position = new_position\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "# Generate expert data (nt trajectories)\n",
    "def generate_expert_data(agent, nt):\n",
    "    trajectories = []\n",
    "    grid_size = agent.gridworld.size\n",
    "    # Set the gridworld payoffs to true parameters\n",
    "    agent.gridworld.payoffs = {1: 1, 2: 0.95, 3: -1}\n",
    "    V_true = agent.value_iteration()\n",
    "    for _ in range(nt):\n",
    "        start_position = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "        trajectory = []\n",
    "        position = start_position\n",
    "        for step in range(10):  # Assuming max_steps=10\n",
    "            probs = agent.get_action_probabilities(position, V_true)\n",
    "            action = np.random.choice(agent.actions, p=probs)\n",
    "            new_position = agent.move(position, action)\n",
    "            reward = agent.gridworld.get_payoff(new_position)\n",
    "            trajectory.append((position, action, reward))\n",
    "            position = new_position\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "\n",
    "# NFXP Algorithm with corrected likelihood computation\n",
    "def nfxp_log_likelihood(params, agent, trajectories):\n",
    "    # Set gridworld payoffs based on parameters\n",
    "    agent.gridworld.payoffs = {1: params[0], 2: params[1], 3: params[2]}\n",
    "    # Solve for value function V\n",
    "    V = agent.value_iteration()\n",
    "    # Compute log-likelihood\n",
    "    log_likelihood = 0\n",
    "    for trajectory in trajectories:\n",
    "        for (position, action, reward) in trajectory:\n",
    "            probs = agent.get_action_probabilities(position, V)\n",
    "            action_index = agent.actions.index(action)\n",
    "            # Add a small epsilon to prevent log(0)\n",
    "            log_likelihood += np.log(probs[action_index] + 1e-12)\n",
    "    return -log_likelihood  # Negative log-likelihood for minimization\n",
    "\n",
    "\n",
    "\n",
    "def estimate_nfxp(agent, trajectories):\n",
    "    initial_params = [0.5, 0.5, -0.5]  # Alternative initial parameters\n",
    "    bounds = [(-2, 2), (-2, 2), (-2, 2)]  # Adjusted bounds\n",
    "    result = minimize(\n",
    "        nfxp_log_likelihood,\n",
    "        initial_params,\n",
    "        args=(agent, trajectories),\n",
    "        bounds=bounds,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': 1000, 'disp': False}\n",
    "    )\n",
    "    estimated_params = result.x\n",
    "    return estimated_params\n",
    "\n",
    "\n",
    "# Run experiments\n",
    "def run_experiment(nt_list=[10, 15, 20], num_trials=20):\n",
    "    gridworld = GridWorld()\n",
    "    agent = Agent(gridworld)\n",
    "    results = defaultdict(list)\n",
    "    for nt in nt_list:\n",
    "        for trial in range(num_trials):\n",
    "            trajectories = generate_expert_data(agent, nt)\n",
    "            # NFXP estimation\n",
    "            nfxp_estimates = estimate_nfxp(agent, trajectories)\n",
    "            results['nfxp'].append((nt, trial, 'preference_shocks', nfxp_estimates))\n",
    "            print(f\"Completed nt={nt}, trial={trial+1}/{num_trials}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to compute mean estimates and errors\n",
    "def compute_mean_estimates(df, algorithm_name):\n",
    "    # True parameter values\n",
    "    true_params = np.array([1, 0.95, -1])\n",
    "    # Process preference_shocks\n",
    "    pref_shocks_df = df[df['process_type'] == 'preference_shocks']\n",
    "    mean_estimates_pref = pref_shocks_df.groupby('nt')[['Feature1', 'Feature2', 'Feature3']].mean()\n",
    "    mean_estimates_pref['Process_Type'] = 'Preference Shocks'\n",
    "    # Compute errors\n",
    "    for i, feature in enumerate(['Feature1', 'Feature2', 'Feature3']):\n",
    "        mean_estimates_pref[f'Error_{feature}'] = mean_estimates_pref[feature] - true_params[i]\n",
    "    print(f\"\\n{algorithm_name} - Mean Estimated Parameters under Preference Shocks:\")\n",
    "    print(mean_estimates_pref[['Feature1', 'Feature2', 'Feature3']])\n",
    "    print(f\"\\n{algorithm_name} - Estimation Errors under Preference Shocks:\")\n",
    "    print(mean_estimates_pref[[f'Error_Feature1', f'Error_Feature2', f'Error_Feature3']])\n",
    "    \n",
    "    # Plotting\n",
    "    for feature in ['Feature1', 'Feature2', 'Feature3']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        subset = mean_estimates_pref\n",
    "        plt.plot(subset.index, subset[feature], marker='o', label='Preference Shocks')\n",
    "        plt.title(f'{algorithm_name} - Mean Estimated {feature}')\n",
    "        plt.xlabel('Number of Trajectories (nt)')\n",
    "        plt.ylabel(f'Mean Estimated {feature}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_experiment()\n",
    "    \n",
    "    # Access NFXP estimates\n",
    "    nfxp_results = results['nfxp']\n",
    "    nfxp_df = pd.DataFrame(nfxp_results, columns=['nt', 'trial', 'process_type', 'estimates'])\n",
    "    nfxp_df[['Feature1', 'Feature2', 'Feature3']] = pd.DataFrame(nfxp_df['estimates'].tolist(), index=nfxp_df.index)\n",
    "    nfxp_df.drop('estimates', axis=1, inplace=True)\n",
    "    \n",
    "    # Compute and display mean estimates for NFXP\n",
    "    compute_mean_estimates(nfxp_df, 'NFXP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd11bc-5591-40c6-a190-ce29862f8ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
